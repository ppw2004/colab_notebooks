{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ppw2004/colab_notebooks/blob/main/Evo_KPN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01ec5b7c",
      "metadata": {
        "id": "01ec5b7c"
      },
      "source": [
        "1. ç¯å¢ƒé…ç½®ä¸ä¾èµ–å®‰è£…\n",
        "å¦‚æœåœ¨ Colab æˆ–æ–°ç¯å¢ƒä¸­è¿è¡Œï¼Œè¯·å–æ¶ˆæ³¨é‡Šå¹¶æ‰§è¡Œå®‰è£…å‘½ä»¤ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b5f5758",
      "metadata": {
        "id": "2b5f5758"
      },
      "outputs": [],
      "source": [
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "# !pip install transformers accelerate peft datasets safetensors scipy scikit-learn pandas\n",
        "# !pip install flash-attn --no-build-isolation  # å¼ºçƒˆæ¨èå®‰è£…ï¼Œæ˜¾è‘—åŠ é€Ÿ Evo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ba2ad87",
      "metadata": {
        "id": "7ba2ad87"
      },
      "source": [
        "2. å¯¼å…¥åº“ä¸å‚æ•°é…ç½®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0604ddd4",
      "metadata": {
        "id": "0604ddd4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# =================== æ ¸å¿ƒé…ç½® ===================\n",
        "CONFIG = {\n",
        "    # è·¯å¾„è®¾ç½®\n",
        "    \"dataset_path\": \"./data/processed_datasets/dataset_meropenem_8k.parquet\", # å…ˆç”¨ 8k è°ƒè¯•ï¼Œè·‘é€šåæ¢ 131k\n",
        "    \"model_id\": \"togethercomputer/evo-1-8k-base\",  # å¯¹åº”æ•°æ®é›†ç‰ˆæœ¬ (8k æˆ– 131k)\n",
        "    \"output_dir\": \"./results/evo_kpn_lora\",\n",
        "\n",
        "    # è®­ç»ƒå‚æ•°\n",
        "    \"max_length\": 8192,      # å¿…é¡»ä¸æ•°æ®é›†åˆ‡ç‰‡é•¿åº¦ä¸€è‡´ (8192 æˆ– 131072)\n",
        "    \"batch_size\": 4,         # æ ¹æ®æ˜¾å­˜è°ƒæ•´ (A100å¯è®¾å¤§ï¼Œ3090éœ€è®¾å°)\n",
        "    \"grad_accum\": 8,         # æ¢¯åº¦ç´¯ç§¯ï¼Œæ¨¡æ‹Ÿå¤§ Batch Size\n",
        "    \"learning_rate\": 1e-4,   # LoRA é€šå¸¸éœ€è¦æ¯”å…¨é‡å¾®è°ƒæ›´å¤§çš„ LR [cite: 85]\n",
        "    \"epochs\": 3,\n",
        "    \"lora_rank\": 16,\n",
        "    \"lora_alpha\": 32,\n",
        "\n",
        "    # æŸå¤±å‡½æ•°å‚æ•°\n",
        "    \"focal_gamma\": 2.0,      # Focal Loss å…³æ³¨éš¾åˆ†æ ·æœ¬çš„ç¨‹åº¦\n",
        "    \"beta\": 0.9999           # Class Balanced Loss çš„å¹³æ»‘å› å­\n",
        "}\n",
        "\n",
        "# æ£€æŸ¥ GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ğŸš€ Running on: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    # å¯ç”¨ FlashAttention\n",
        "    torch.backends.cuda.enable_flash_sdp(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f765c13",
      "metadata": {
        "id": "8f765c13"
      },
      "source": [
        "3. æ•°æ®åŠ è½½ä¸é˜²æ³„æ¼åˆ’åˆ†\n",
        "å…³é”®æ­¥éª¤ï¼šä¸ºäº†é˜²æ­¢â€œæ•°æ®æ³„éœ²â€ï¼Œæˆ‘ä»¬å¿…é¡»æŒ‰ genome_id è¿›è¡Œåˆ’åˆ†ã€‚ç»ä¸èƒ½ç®€å•éšæœºåˆ’åˆ†ï¼Œå¦åˆ™åŒä¸€ä¸ªç»†èŒçš„åˆ‡ç‰‡ä¼šåŒæ—¶å‡ºç°åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­ï¼Œå¯¼è‡´æ¨¡å‹â€œèƒŒé¢˜â€ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba0b0bbd",
      "metadata": {
        "id": "ba0b0bbd"
      },
      "outputs": [],
      "source": [
        "def load_and_split_data(parquet_path):\n",
        "    print(f\"ğŸ“– Loading data from {parquet_path}...\")\n",
        "    df = pd.read_parquet(parquet_path)\n",
        "\n",
        "    # ç®€å•çš„ EDA\n",
        "    n_pos = sum(df['label'] == 1)\n",
        "    n_neg = sum(df['label'] == 0)\n",
        "    print(f\"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(df)}\")\n",
        "    print(f\"   è€è¯ (1): {n_pos} ({n_pos/len(df):.2%})\")\n",
        "    print(f\"   æ•æ„Ÿ (0): {n_neg} ({n_neg/len(df):.2%})\")\n",
        "\n",
        "    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°ï¼Œä¾› Loss å‡½æ•°ä½¿ç”¨\n",
        "    samples_per_cls = [n_neg, n_pos]\n",
        "\n",
        "    # --- Group Shuffle Split (é˜²æ³„æ¼) ---\n",
        "    splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "    train_idx, test_idx = next(splitter.split(df, groups=df['genome_id']))\n",
        "\n",
        "    train_df = df.iloc[train_idx]\n",
        "    test_df = df.iloc[test_idx]\n",
        "\n",
        "    print(f\"âœ… åˆ’åˆ†å®Œæˆ (æŒ‰ Genome ID):\")\n",
        "    print(f\"   Train set: {len(train_df)} slices (Genomes: {train_df['genome_id'].nunique()})\")\n",
        "    print(f\"   Test set:  {len(test_df)} slices (Genomes: {test_df['genome_id'].nunique()})\")\n",
        "\n",
        "    # è½¬æ¢ä¸º HuggingFace Dataset\n",
        "    hf_train = Dataset.from_pandas(train_df.drop(columns=['__index_level_0__'], errors='ignore'))\n",
        "    hf_test = Dataset.from_pandas(test_df.drop(columns=['__index_level_0__'], errors='ignore'))\n",
        "\n",
        "    return DatasetDict({\"train\": hf_train, \"test\": hf_test}), samples_per_cls\n",
        "\n",
        "dataset, samples_per_cls = load_and_split_data(CONFIG[\"dataset_path\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4546379",
      "metadata": {
        "id": "c4546379"
      },
      "source": [
        "4. æ•°æ®é¢„å¤„ç† (Tokenization)\n",
        "Evo ä½¿ç”¨å­—èŠ‚çº§ Tokenizerï¼Œæˆ‘ä»¬éœ€è¦å°† DNA å­—ç¬¦ä¸²è½¬æ¢ä¸º Input IDsã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0975abc5",
      "metadata": {
        "id": "0975abc5"
      },
      "outputs": [],
      "source": [
        "# åŠ è½½ Evo çš„ Tokenizer (é€šå¸¸ Evo ä½¿ç”¨è‡ªå®šä¹‰çš„ï¼Œè¿™é‡Œæˆ‘ä»¬æ¨¡æ‹Ÿ byte-level å¤„ç†)\n",
        "# æ³¨æ„ï¼šEvo å®˜æ–¹æ¨¡å‹é€šå¸¸ä¸éœ€è¦ç”± transformers æä¾›çš„ AutoTokenizerï¼Œ\n",
        "# ä½†ä¸ºäº†å…¼å®¹ Trainerï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªç®€å•çš„è½¬æ¢å‡½æ•°\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # Evo çš„è¾“å…¥é€šå¸¸æ˜¯ bytes çš„ ASCII ç¼–ç \n",
        "    # æˆ‘ä»¬éœ€è¦å°†å­—ç¬¦ä¸² \"ACGT...\" è½¬æ¢ä¸ºå¯¹åº”çš„ tensor\n",
        "    # Evo è¯è¡¨è®¾è®¡ï¼šé€šå¸¸ç›´æ¥æ˜ å°„ bytes\n",
        "\n",
        "    sequences = examples[\"sequence\"]\n",
        "\n",
        "    # ç®€å•çš„ Byte-level ç¼–ç  (Evo å…¼å®¹)\n",
        "    # åŠ ä¸Š batch ç»´åº¦å¤„ç†\n",
        "    input_ids = []\n",
        "    for seq in sequences:\n",
        "        # å°†å­—ç¬¦ä¸²è½¬ä¸º ASCII ç åˆ—è¡¨ (0-255)\n",
        "        ids = [ord(c) for c in seq]\n",
        "        # æˆªæ–­æˆ– Padding åœ¨ collator ä¸­å¤„ç†ï¼Œæˆ–è€…è¿™é‡Œåš\n",
        "        if len(ids) > CONFIG[\"max_length\"]:\n",
        "            ids = ids[:CONFIG[\"max_length\"]]\n",
        "        input_ids.append(ids)\n",
        "\n",
        "    return {\"input_ids\": input_ids}\n",
        "\n",
        "print(\"ğŸ”„ Tokenizing data...\")\n",
        "tokenized_datasets = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"sequence\", \"genome_id\", \"chunk_id\"], # ç§»é™¤ä¸éœ€è¦çš„åˆ—\n",
        "    num_proc=4\n",
        ")\n",
        "print(\"âœ… Tokenization complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0c0a740",
      "metadata": {
        "id": "b0c0a740"
      },
      "source": [
        "5. è‡ªå®šä¹‰æ¨¡å‹æ¶æ„ (EvoClassifier)\n",
        "è¿™æ˜¯å®éªŒæ–¹æ¡ˆçš„æ ¸å¿ƒï¼šBackbone + Last Token Pooling + Classifierã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11f0a606",
      "metadata": {
        "id": "11f0a606"
      },
      "outputs": [],
      "source": [
        "class EvoClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_labels=2, lora_config=None):\n",
        "        super().__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "        print(f\"ğŸ—ï¸ Loading Backbone: {model_name}\")\n",
        "        self.backbone = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            config=self.config,\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.bfloat16, # ä½¿ç”¨ BF16 é˜²æ­¢æº¢å‡º [cite: 82]\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        # å¯ç”¨ Gradient Checkpointing èŠ‚çœæ˜¾å­˜\n",
        "        self.backbone.gradient_checkpointing_enable()\n",
        "\n",
        "        # åº”ç”¨ LoRA\n",
        "        if lora_config:\n",
        "            print(\"ğŸ”§ Applying LoRA...\")\n",
        "            self.backbone = get_peft_model(self.backbone, lora_config)\n",
        "            self.backbone.print_trainable_parameters()\n",
        "\n",
        "        # è‡ªå®šä¹‰åˆ†ç±»å¤´ [cite: 41]\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.config.hidden_size, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(1024, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        # Evo forward\n",
        "        outputs = self.backbone(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        # Last Token Pooling [cite: 40]\n",
        "        # æå–æœ€åä¸€å±‚çš„ hidden states\n",
        "        last_hidden_state = outputs.hidden_states[-1]\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # æ‰¾åˆ°æœ€åä¸€ä¸ªé padding çš„ token\n",
        "            last_token_indices = attention_mask.sum(dim=1) - 1\n",
        "            batch_size = last_hidden_state.shape[0]\n",
        "            pooled_output = last_hidden_state[torch.arange(batch_size), last_token_indices]\n",
        "        else:\n",
        "            pooled_output = last_hidden_state[:, -1, :]\n",
        "\n",
        "        # é¢„æµ‹\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # Loss è®¡ç®—å°†åœ¨ Trainer ä¸­é€šè¿‡ CustomTrainer å¤„ç†\n",
        "            # è¿™é‡Œä»…è¿”å› logits ä¾› Trainer ä½¿ç”¨ï¼Œæˆ–è€…ç›´æ¥åœ¨è¿™é‡Œç®—æ ‡å‡† CrossEntropy\n",
        "            # ä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬è¿”å› logits\n",
        "            pass\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5084084f",
      "metadata": {
        "id": "5084084f"
      },
      "source": [
        "6. æŸå¤±å‡½æ•°ï¼šClass-Balanced Focal Loss\n",
        "å®ç°æ–¹æ¡ˆä¸­æåˆ°çš„ CB-Focal Loss ï¼Œä¸“é—¨å¯¹ä»˜ä¸å¹³è¡¡æ•°æ®ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ada16ac4",
      "metadata": {
        "id": "ada16ac4"
      },
      "outputs": [],
      "source": [
        "class CBFocalLoss(nn.Module):\n",
        "    def __init__(self, samples_per_cls, no_of_classes=2, gamma=2.0, beta=0.9999):\n",
        "        super(CBFocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.samples_per_cls = samples_per_cls\n",
        "        self.no_of_classes = no_of_classes\n",
        "\n",
        "        # è®¡ç®— Class-Balanced Weights\n",
        "        effective_num = 1.0 - np.power(beta, samples_per_cls)\n",
        "        weights = (1.0 - beta) / np.array(effective_num)\n",
        "        weights = weights / np.sum(weights) * no_of_classes\n",
        "\n",
        "        self.weights = torch.tensor(weights).float().to(device)\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        # Focal Loss logic\n",
        "        ce_loss = F.cross_entropy(logits, labels, reduction='none', weight=self.weights)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "# å®ä¾‹åŒ– Loss\n",
        "cb_focal_loss_fn = CBFocalLoss(samples_per_cls=samples_per_cls, gamma=CONFIG[\"focal_gamma\"], beta=CONFIG[\"beta\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5821251",
      "metadata": {
        "id": "d5821251"
      },
      "source": [
        "7. è‡ªå®šä¹‰ Trainer\n",
        "è¦†ç›– compute_loss æ–¹æ³•ï¼Œæ³¨å…¥æˆ‘ä»¬çš„ CB-Focal Lossã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9920d51b",
      "metadata": {
        "id": "9920d51b"
      },
      "outputs": [],
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        # forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        # ä½¿ç”¨è‡ªå®šä¹‰ Loss\n",
        "        loss = cb_focal_loss_fn(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dcdfc31",
      "metadata": {
        "id": "7dcdfc31"
      },
      "source": [
        "8. è¯„ä¼°æŒ‡æ ‡ (Metrics)\n",
        "é‡ç‚¹å…³æ³¨ AUPRC ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0484eddb",
      "metadata": {
        "id": "0484eddb"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    # Softmax è·å–æ¦‚ç‡\n",
        "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)[:, 1].numpy()\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # è®¡ç®—æŒ‡æ ‡\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    auroc = roc_auc_score(labels, probs)\n",
        "    auprc = average_precision_score(labels, probs) # æ ¸å¿ƒæŒ‡æ ‡\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'auroc': auroc,\n",
        "        'auprc': auprc  # é‡ç‚¹ç›‘æ§\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a20d96c",
      "metadata": {
        "id": "0a20d96c"
      },
      "source": [
        "9. å¼€å§‹è®­ç»ƒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9271291e",
      "metadata": {
        "id": "9271291e"
      },
      "outputs": [],
      "source": [
        "# LoRA é…ç½®\n",
        "lora_config = LoraConfig(\n",
        "    r=CONFIG[\"lora_rank\"],\n",
        "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
        "    # StripedHyena çš„ç‰¹å®šå±‚åç§° [cite: 51]\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.FEATURE_EXTRACTION # æˆ–ä¸æŒ‡å®šï¼Œå› ä¸ºæ˜¯è‡ªå®šä¹‰æ¨¡å‹\n",
        ")\n",
        "\n",
        "# åˆå§‹åŒ–æ¨¡å‹\n",
        "model = EvoClassifier(CONFIG[\"model_id\"], num_labels=2, lora_config=lora_config)\n",
        "model.to(device)\n",
        "\n",
        "# Data Collator (å¤„ç† Padding)\n",
        "# è¿™é‡Œçš„ tokenizer æ²¡æœ‰ pad_tokenï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®šã€‚Evo é€šå¸¸ç”¨ N æˆ–è€…ç‰¹æ®Š token\n",
        "# ç®€å•çš„ Byte level padding å¯ä»¥ç”¨ 0 æˆ–å…¶ä»–\n",
        "data_collator = DataCollatorWithPadding(tokenizer=AutoTokenizer.from_pretrained(CONFIG[\"model_id\"], trust_remote_code=True))\n",
        "# æ³¨æ„ï¼šå¦‚æœ AutoTokenizer æŠ¥é”™ï¼Œéœ€æ‰‹åŠ¨å®ç° collate_fnï¼Œå°† list of input_ids pad åˆ°ç›¸åŒé•¿åº¦\n",
        "\n",
        "# è®­ç»ƒå‚æ•°\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=CONFIG[\"output_dir\"],\n",
        "    learning_rate=CONFIG[\"learning_rate\"],\n",
        "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
        "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
        "    gradient_accumulation_steps=CONFIG[\"grad_accum\"],\n",
        "    num_train_epochs=CONFIG[\"epochs\"],\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100, # æ¯100æ­¥éªŒè¯ä¸€æ¬¡\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"auprc\", # ä»¥ AUPRC ä¸ºå‡†\n",
        "    greater_is_better=True,\n",
        "    bf16=True, # å¿…é¡»ä½¿ç”¨ BF16 [cite: 82]\n",
        "    dataloader_num_workers=4,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# åˆå§‹åŒ– Trainer\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    # data_collator=data_collator # å¦‚æœå‰é¢ tokenized å·²ç»æ˜¯å®šé•¿ï¼Œè¿™é‡Œå¯ä»¥ä¸ç”¨\n",
        ")\n",
        "\n",
        "print(\"ğŸ”¥ Starting Training...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58d1936d",
      "metadata": {
        "id": "58d1936d"
      },
      "source": [
        "10. æ¨¡å‹ä¿å­˜ä¸æ¨ç†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a8af24",
      "metadata": {
        "id": "c7a8af24"
      },
      "outputs": [],
      "source": [
        "# ä¿å­˜å¾®è°ƒåçš„é€‚é…å™¨\n",
        "model.backbone.save_pretrained(os.path.join(CONFIG[\"output_dir\"], \"final_lora_adapter\"))\n",
        "torch.save(model.classifier.state_dict(), os.path.join(CONFIG[\"output_dir\"], \"classifier_head.pth\"))\n",
        "print(\"ğŸ’¾ Model saved.\")\n",
        "\n",
        "# æ¨ç†ç¤ºä¾‹ (Inference)\n",
        "def predict_genome(sequence):\n",
        "    model.eval()\n",
        "    # Tokenize\n",
        "    input_ids = [ord(c) for c in sequence]\n",
        "    input_tensor = torch.tensor([input_ids]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        probs = torch.nn.functional.softmax(outputs['logits'], dim=-1)\n",
        "\n",
        "    return probs[0][1].item() # è¿”å›è€è¯æ¦‚ç‡\n",
        "\n",
        "# æµ‹è¯•ä¸€æ¡\n",
        "print(f\"Test Prediction (Resistant Prob): {predict_genome('ATCG'*100):.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}